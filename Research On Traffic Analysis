1.	Introduction With the development of smart cities, intelligent transportation systems (ITS) have been deployed around the world to fix or alleviate the problems associated with traffic. Urban traffic management systems collect data about the state of traffic from the urban traffic network, such as volume, occupancy and speed. This data can come from a variety of sources, such as GPS systems, loop detectors, radio frequency identification (RFID), or other systems
2.	 There are many models that attempt to predict vehicular speed in urban and interurban roads, the noise pollution caused by traffic in cities, or even the traffic flow based on historical data from cameras or from people’s mobile phones. Such information can be useful for administration authorities, and for researchers attempting to improve the living conditions of citizens. In this context, the aim of the present study is to design a model capable of predicting the traffic flow in the city of Valencia, Spain, based on data collected by electromagnetic loops distributed throughout the city. With a good traffic prediction, it will be possible to foresee possible traffic jams, and also to trigger countermeasures to mitigate them. Therefore, two models based on two recurrent neural networks of Long Short-Term Memory (LSTM) type have been designed to predict the traffic flow in the different streets of Valencia at the different hours of the day. We also study the influence of the specific characteristics used on the accuracy of the model. The results of our experiments show that, despite the high heterogeneity in terms of per-street traffic behaviour, it is possible to reach useful prediction models with low errors
3.	 Keywords: Forecasting; traffic flow; time series; deep learning; LSTM. 
4.	 Introduction With the development of smart cities, intelligent transportation systems (ITS) have been deployed around the world to fix or alleviate the problems associated with traffic . Urban traffic management systems collect data about the state of traffic from the urban traffic network, such as volume, occupancy and speed. This data can come from a variety of sources, such as GPS systems, loop detectors, radio frequency identification (RFID), or other .
5.	There are many models that attempt to predict vehicular speed in urban and interurban roads, the noise pollution caused by traffic in cities, or even the traffic flow based on historical data from cameras or from people’s mobile phones. Such information can be useful for administration authorities, and for researchers attempting to improve the living conditions of citizens. In this context, the aim of the present study is to design a model capable of predicting the traffic flow in the city of Valencia, Spain, based on data collected by electromagnetic loops distributed throughout the city. With a good traffic prediction, it will be possible to foresee possible traffic jams, and also to trigger countermeasures to mitigate them. Therefore, two models based on two recurrent neural networks of Long Short-Term Memory (LSTM) type have been designed to predict the traffic flow in the different streets of Valencia at the different hours of the day. We also study the influence of the specific characteristics used on the accuracy of the model. 
6.	Traffic flow prediction is a fundamental issue for researchers and also a challenging one, as traffic flow is often highly non-linear and complex in its patterns . In addition, out-of-the-ordinary events, such as this year’s pandemic, or something more common, including mass events like a concert or a football match, can drastically alter the normal behaviour of traffic flow . Being able to predict traffic flow in large cities provides a great benefit in multiple areas, such as vehicle routing or traffic congestion management. Good traffic flow prediction can help to avoid traffic congestion by being able to reroute a vehicle to a less congested route. A particularly useful technique for predicting traffic flow is to rely on long short-term memory (LSTM) networks . LSTM networks are a type of recurrent neural networks that are suitable for modelling serial data such as time series. One of their main advantages is that they are able to process long sequences of information, such as traffic flow. For this reason, in the scope of this work, we find that they can be particularly useful when targeting the changing behaviour of traffic flow levels in different streets along the day. Traffic prediction with LSTM has gained attention lately due to the ability of LSTM to deal with time long series and calculate optimal time lags . Most of the works apply traffic prediction to limited areas such as a road network with four expressways , a few locations representing several entry and exit reference points between neighbourhoods in New York City , or twenty-one loop-detectors installed on a northbound section of an interstate in Chicago .
7.	 In contrast, in this work we propose the application of a Deep Learning approach to predict the traffic flow of an entire city with a population of almost 1,5 million people. This paper proposes two solutions that allow predicting, for each hour of the day, the traffic flow in the streets of the city of Valencia, Spain. To this end, two novel short-term traffic flow prediction models are defined based on recurrent neural networks; specifically, we rely on LSTMs to create our models, and on metrics like the Mean Absolute Percentage Error (MAPE) and the Symmetric Mean Absolute Percentage Error (sMAPE) to validate the quality of our models. Notice that we discard using the MSE metric as it is scale dependant, and in our case we have streets with very heterogeneous traffic flow levels, and so we have decided to use MAPE and sMAPE only. The remainder of this paper is organised as follows: a brief overview of related work, and an explanation of time series, take place. Then, a characterisation of available data is presented in. details the methodology followed in this work. The evaluation of the proposed models is then performed in, including the results’ discussion. Finally, presents the main conclusions, and refers to future research works

Background and Related work In this section we provide some background on relevant areas for this work: 
(i)	time series, 
(ii)	Autoregressive integrated moving average (ARIMA) models, 
(iii)	Neural network-based models, and 
(iv)	mixed models.. Time series A time series is defined as a collection of observations of a variable collected sequentially over time; these observations are usually collected at equidistant time intervals. This is intended not only to explain past events, but also to be able to predict the future by looking at past samples. Time series are usually modelled by a stochastic process, i.e, a sequence of random variables. Typically, by knowing the value of a variable at a time t, the value of that variable at a time t + 1 can be predicted. There are three important aspects to take into account when defining a time series, and these are stationarity, seasonality, and autocorrelation. A time series is said to be stationary if its statistical properties do not change over time, i.e, it has a constant mean and variance. Furthermore, within stationarity, we study the trend of the data, which can be defined as a long-run change that occurs relative to the mean of the data, or the long-run change in the mean. In other words, the trend shows whether the data is increasing, decreasing, or remaining constant over time. On the other hand, seasonality refers to periodic fluctuations in the data; for example, electricity consumption is normally high during the day, and low at night. These fluctuations can occur at different time levels, and can be hourly, daily, weekly, monthly, etc. Finally, autocorrelation refers to the similarity between observations as a function of the time lag between them, i.e, how closely related a time series is to a time lagged version of itself.
(v)	Another important part is the decomposition of the time series. Two types of decomposition models can be done:
(i)	multiplicative models where the time series is the result of multiplying the elements,  additive models, which are the result of adding the element of the series. The additive model follows the equation: Yt = Tt + S t + et and the multiplicative model: Yt = Tt · S t · et where Y is the time series, T the trend, S the seasonality, and e the residual. Trend and seasonality have been discussed above, while the residual is the result of subtracting the seasonality and trend from the time series or dividing them, depending on the type of decomposition model used. ARIMA models The ARIMA model is a dynamic model using time series data developed by Box and Jenkins in 1970 [23]. The ARIMA model allows a value to be described as a linear function of past data and random errors and, in addition, allows for the inclusion of a cyclical or seasonal component. ARIMA models have been widely applied in traffic forecasting problems like predicting the traffic flow in a road of Beijing, China [7], where authors point that most of the state-of-the-art work trains the models with the full time series, without splits. However, they think that the time series should be split, so they have divided the time series into different days. In addition, within the days, they difference three periods: the morning peak period, the evening peak period and the normal period (non-peak period). They conclude their work by comparing their model with a model without splitting the time series, where it can be seen that they have been able to improve the results. The authors in [13] explain that one of the problems of using ARIMA models is that they require a solid database for the construction of the model. Therefore, they propose a solution to this problem by using a seasonal ARIMA model (SARIMA) [11] for short-term traffic flow prediction with limited input data. SARIMA models are an extension of ARIMA models which capture the purely seasonal behaviour of a series, and do not have to be removed from the data, as with ARIMA models.
Methodology ➢ Description of the data sets used for the analysis: ● Traffic Flow Data Set: This data set contains traffic flow data collected from sensors installed on a busy highway over a period of one month. The data set includes variables such as the time of day, day of the week, traffic volume, and weather conditions. We used this data set to train and test our machine learning models for predicting traffic volume.

Explain the dataset used and the criteria for selecting the algorithms: 
The dataset used in our analysis for AI in traffic management is a collection of traffic data obtained from various sources, including traffic cameras, sensors, and GPS devices. The data includes information such as vehicle speed, location, and direction, as well as road conditions, weather, and other relevant factors that may affect traffic flow. The dataset was chosen based on its completeness, diversity, and relevance to real-world traffic management scenarios. In selecting the algorithms to analyze, we considered a variety of factors, including their performance on similar datasets, their complexity, interpretability, and computational efficiency. We also took into account the specific needs and requirements of traffic management, such as the ability to predict traffic congestion, optimize traffic flow, and adapt to changing conditions in real-time. Based on these criteria, we selected a range of machine learning algorithms, including decision trees, random forests, support vector machines, neural networks, and deep learning models.

Discuss the parameters used to evaluate the algorithms: The evaluation of machine learning algorithms for traffic management typically involves several parameters, including: 
1. Accuracy: This parameter measures the ability of the algorithm to correctly predict traffic patterns and conditions. It is usually evaluated by comparing the algorithm's output to real-world traffic data. 
2. Efficiency: This parameter measures the speed and computational complexity of the algorithm. It is important to consider the efficiency of the algorithm since it determines how quickly it can process large amounts of data and provide accurate predictions.
 3. Robustness: This parameter measures the algorithm's ability to perform well under different conditions and environments. It is essential to evaluate the algorithm's robustness since traffic patterns and conditions can vary significantly depending on the location and time of day. 
4. Scalability: This parameter measures the ability of the algorithm to handle larger and more complex datasets. As traffic patterns become more complex, it is important to evaluate the algorithm's ability to scale up and handle the additional data.

Describe the methodology used to analyze the effectiveness of machine learning algorithms in managing traffic: To analyze the effectiveness of machine learning algorithms in managing traffic, the following methodology was used: 
1. Data collection: Traffic data was collected from various sources, including traffic sensors, cameras, and GPS-enabled devices. The data included information on traffic volume, speed, and congestion levels. 
2. Data pre-processing: The collected data was pre-processed to remove any inconsistencies or errors. The data was also normalized to ensure that all features had equal weightage in the analysis. 
3. Algorithm selection: Various machine learning algorithms were selected for the analysis, including decision trees, random forests, support vector machines, and neural networks. The selection was based on their ability to handle large datasets, their interpretability, and their performance in previous studies.

Discuss the implications of the research findings for the future of traffic management and the use of AI: The research findings have significant implications for the future of traffic management and the use of AI. The study demonstrates that machine learning algorithms can effectively predict traffic congestion and provide real-time traffic management solutions. This could potentially lead to the development of smarter and more efficient transportation systems that can adapt to changing traffic conditions. The findings also suggest that the use of AI can help reduce traffic congestion, travel time, and fuel consumption, leading to a decrease in greenhouse gas emissions and air pollution. Moreover, by reducing the number of accidents caused by traffic congestion, AI can improve road safety

One of the strengths of the Decision Trees algorithm is its simplicity and ability to handle both categorical and continuous data. However, it tends to overfit the data, which can lead to reduced accuracy on new data. Random Forest, on the other hand, is an ensemble method that creates multiple decision trees and combines their outputs, which helps to reduce overfitting and improve accuracy. It also has the advantage of being able to handle large datasets with many features. Support Vector Machines are effective at handling datasets with high-dimensional features and can work well with both linear and nonlinear data. However, they can be sensitive to the choice of kernel function and require more computational resources. Overall, our results suggest that Random Forest is the most effective algorithm for predicting traffic congestion levels in our dataset. However, the choice of algorithm may vary depending on the specific characteristics of the dataset and the goals of the traffic management system.
